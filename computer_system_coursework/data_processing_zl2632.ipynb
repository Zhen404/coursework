{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "***\n",
    "* Assigned: 02/07\n",
    "* Due: **02/20 at 11:59pm** electronically\n",
    "* This assignment is worth 100 points.\n",
    "\n",
    "**Please do read the instructions for each response carefully, since a whole lot of different variety of reponses are involved in the assignment (including pasting code snippets, writing executable code, small writeups). You don't wanna be losing points for silly errors.**\n",
    "\n",
    "### Jupyter Notes:\n",
    "\n",
    "* You **may** create new IPython notebook cells to use for e.g. testing, debugging, exploring, etc.- this is encouraged in fact!\n",
    "  * you can press shift+enter to execute the code in the cell that your cursor is in.\n",
    "* When you see `In [*]:` to the left of the cell you are executing, this means that the code / query is _running_. Please wait for the execution to complete\n",
    "    * **If the cell is hanging- i.e. running for too long: you can restart the kernel**\n",
    "    * To restart kernel using the menu bar: \"Kernel >> Restart >> Clear all outputs & restart\"), then re-execute cells from the top\n",
    "* _Have fun!_\n",
    "\n",
    "\n",
    "### Setup Your Credentials\n",
    "\n",
    "Update the following variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your columbia uni that is used in SSOL\n",
    "#\n",
    "# IMPORTANT:  make sure this is consistent with the uni/alias used as your @columbia.edu email in SSOL\n",
    "#\n",
    "UNI = \"zl2632\"\n",
    "\n",
    "# your instabase username (if you go to the instabase homepage, your username should be in the URL)\n",
    "USER = \"zhen404\"\n",
    "\n",
    "# your repository name containing \n",
    "REPO = \"CSDS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "In this lab, you will use various types of tools -- from high-level tools like Data Wrangler to command line tools like `sed` and `awk` -- to perform data parsing and extraction from data encoded into a text file.  The goal of this lab is simply to gain experience with these tools and compare and contrast their usage.\n",
    "\n",
    "The `lab` directory contains two datasets that you will work with:\n",
    "\n",
    "1. A dataset of all the movies in 2013 from January to March (`2013films.txt`). It contains Movie name, Production house, Genre, Publisher and some other details.\n",
    "\n",
    "1. The second dataset (`worldcup.txt`) is a snippet of the following Wikipedia webpage on [FIFA (Soccer) World Cup](http://en.wikipedia.org/wiki/FIFA_World_Cup).\n",
    "Specifically it is a partially cleaned-up wiki source for the table toward the end of the page that lists teams finishing in the top 4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 1. Get Wrangling !!\n",
    "***\n",
    "Download the [Trifacta Wrangler](https://www.trifacta.com/products/wrangler/) tool. Load both the datasets into wrangler and try playing around with the tool.\n",
    "\n",
    "Some tips using Wrangler:\n",
    "\n",
    "* Check out the introduction [video](https://vimeo.com/19185801) to get a feel of how wrangler works.\n",
    "* You may wanna start off by loading ~50 lines of data instead of the entire files and play around with the tool.\n",
    "* Wrangler responds to mouse highlights and clicks on the displayed table cells by suggesting operations on the left sidebar.  \n",
    "* Hovering over each element shows the result in the table view.  \n",
    "* Clicking adds the operation.  \n",
    "* Clear the sidebar by clicking the colored row above the schema row.\n",
    "\n",
    "## Tasks:\n",
    "\n",
    "Use Data Wrangler for the following two datasets.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2013films.txt\n",
    "\n",
    "Use the tool to generate output as follows, i.e., Movie name, Production/Distribtuion house, Director, Genre and publisher. \n",
    "\n",
    "        'A dark truth, Magnolia Pictures, Damian Lee, Action,ComingSoon.net\n",
    "         Table No. 21, Eros International, Aditya Datt, Thriller, BoxOfficeMojo.com\n",
    "         ...\n",
    "        \n",
    "For the purpose of explanation columns are separated by ||. You can choose any pattern to extract information. \n",
    "\n",
    "1. Movie name can be identified as first column in every line formatted as ''[[ <movie name> ]]''  \n",
    "1. Production/Distribution house is the following column that is formatted as [[< Production house>]]  \n",
    "1. Director name can be identified with \"(director)\" tag that follows the name  \n",
    "1. Genre is present in the next column but make sure to extract only second part that is separated by | operator. For eg. in [Action film|Action] relevant information is Action  \n",
    "1. Publisher name can be identified in the last column with format \"publisher=<publisher name>\"  \n",
    "1. It may help to skip first few lines that contains html code, so that you process actual records.\n",
    "\n",
    "#### Questions\n",
    "\n",
    "* Use wrangler to clean the data, then determine which Production/Distribution house produced maximum movies.\n",
    "\n",
    "#### Notes\n",
    "* Your wrangler script is not expected to be sophisticated enough to generate the results for the question asked. It just needs to clean/combine the data enough for you to observe the data and answer the question.\n",
    "* You can export the transformations you carried out in wrangler.\n",
    "Export the script and paste it in the cell below. Do not bother executing it here in the notebook.\n",
    "* Stanford also has an online open-source version of Trifacta Wrangler that can be used [here](http://vis.stanford.edu/wrangler/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ans:\n",
    "From the data cleaned by wrangler, Warner Bros. produced maximum movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your wrangler script goes in this cell below\n",
    "split col: column1 on: `\\|\\|` limit: 4\n",
    "delete row: ismissing([column3])\n",
    "extract col: column2 after: `\\[` before: `\\]`\n",
    "drop col: column2\n",
    "extract col: column3 after: `\\[` before: `\\]`\n",
    "set col: column2 value: ifmissing($col, column3)\n",
    "replace col: column2 with: '' on: `{start}{delim}`\n",
    "drop col: column3\n",
    "extract col: column4 before: `\\(director`\n",
    "extract col: column3 after: `\\[` before: `\\]`\n",
    "set col: column7 value: ifmissing($col, column3)\n",
    "replace col: column7 with: '' on: `{start}{delim}`\n",
    "replace col: column7 with: '' on: `[[` global: true\n",
    "replace col: column7 with: '' on: `{start}{delim}`\n",
    "drop col: column3\n",
    "extract col: column5 limit: 2 after: `\\|` before: `\\]`\n",
    "merge col: column3,column8 with: '' as: 'column9'\n",
    "drop col: column3\n",
    "drop col: column8\n",
    "extract col: column6 after: `publisher=` before: `\\|`\n",
    "replace col: column3 with: '' on: ` ` global: true\n",
    "replace col: column3 with: '' on: `[[` global: true\n",
    "drop col: column6\n",
    "drop col: column5\n",
    "drop col: column4\n",
    "rename col: column1 to: 'Moviename'\n",
    "rename col: column2 to: 'ProductionHouse'\n",
    "rename col: column7 to: 'Director'\n",
    "rename col: column9 to: 'Genre'\n",
    "rename col: column3 to: 'Publisher'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### worldcup.txt\n",
    "\n",
    "Use the tool to generate output as follows, i.e., each line in the output contains a country, a year, and the position of the county in that year (if within top 4).\n",
    "\n",
    "        BRA, 1962, 1\n",
    "        BRA, 1970, 1\n",
    "        BRA, 1994, 1\n",
    "        BRA, 2002, 1\n",
    "        BRA, 1958, 1\n",
    "        BRA, 1998, 2\n",
    "        BRA, 1950, 2\n",
    "        ...\n",
    "\n",
    "It may help to \n",
    "\n",
    "1. Skip the first 20 or so rows of table headers and other text, so that the data wrangler works with are \"record text\".  \n",
    "2. Delete the rows that are clearly HTML formatting content\n",
    "3. Extract the relevant data from the remaining column into new columns\n",
    "4. Use the fill operation\n",
    "\n",
    "#### Questions\n",
    "\n",
    "* According to the dataset, how often has each country won the world cup?\n",
    "\n",
    "#### Notes\n",
    "* Your wrangler script is not expected to be sophisticated enough to generate the results for the question asked. It just needs to clean/combine the data enough for you to observe the data and answer the question.\n",
    "* You can export the transformations you carried out in wrangler. Export the script and paste it in the cell below. Do not bother executing it here in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ans:\n",
    "I use the number of first place dividing total duration of years of world cup as a measure showing that how often a country won the world cup. From the table we cleaned, BRA is 0.06. ITA, GER are 0.05. ARG, URU are 0.02, FRA, ENG, ESP are 0.01. NED, TCH, HUN, SWE, POL, AUT, POR, USA, CHI, CRO, TUR, YUG, URS, BEL, BUL, KOR are 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your wrangler script goes in this cell below\n",
    "extract col: column1 after: `\\{` before: `\\}`\n",
    "extract col: column2 after: `fb\\|`\n",
    "drop col: column2\n",
    "extractlist col: column1 on: `\\|{digit}{4}]` as: 'column4'\n",
    "extractlist col: column4 on: `{digit}{4}` as: 'column5'\n",
    "set col: column3 value: fill(ifmissing($col, null())) order: sourcerownumber()\n",
    "delete row: ismissing([column3])\n",
    "derive value: arraylen(column5) as: 'column2'\n",
    "drop col: column4\n",
    "flatten col: column5\n",
    "drop col: column1\n",
    "rename col: column5 to: 'Year'\n",
    "rename col: column3 to: 'Country'\n",
    "rename col: column2 to: 'number_of_place'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 2. Grep, Sed & Awk\n",
    "***\n",
    "\n",
    "The set of three UNIX tools, sed, awk, and grep, can be very useful for quickly cleaning up and transforming data for further analysis (and have been around since the inception of UNIX). In conjunction with other unix utilities like sort, uniq, tail, head, etc., you can accomplish many simple data parsing and cleaning tasks with these tools. You are encouraged to play with these tools and familiarize yourselves with the basic usage of these tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "grep 'regexp' filename\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or equivalently (using UNIX pipelining):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```bash\n",
    "cat filename | grep 'regexp'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output contains only those lines from the file that match the regular expression. Two options to grep are useful: grep -v will output those lines that do not match the regular expression, and grep -i will ignore case while matching. See the manual (man grep) (or online resources) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sed stands for stream editor. Basic syntax for sed is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```bash\n",
    "sed 's/regexp/replacement/g' filename\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each line in the intput, the portion of the line that matches regexp (if any) is replaced with replacement. Sed is quite powerful within the limits of operating on single line at a time. You can use \\( \\) to refer to parts of the pattern match. In the first sed command above, the sub-expression within \\( \\) extracts the user id, which is available to be used in the replacement as \\1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# awk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, awk is a powerful scripting language (not unlike perl). The basic syntax of awk is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```bash\n",
    "awk -F',' 'BEGIN{commands} /regexp1/ {command1} /regexp2/ {command2} END{commands}'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each line, the regular expressions are matched in order, and if there is a match, the corresponding command is executed (multiple commands may be executed for the same line). BEGIN and END are both optional. The -F',' specifies that the lines should be split into fields using the separator \",\", and those fields are available to the regular expressions and the commands as $1, $2, etc. See the manual (man awk) or online resources for further details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "We start off by copying the files from our instabase repository to the VM filesystem our instabse instance is running on.\n",
    "Remember, you'll have to execute the cell below everytime the VM is restarted (happens when you close and restart the notebook) before you can proceed with the bash examples that follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally, the tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use the awk \"split\" function and \"for loop\" constructs on World Cup data, to again generate output as follows, i.e., each line in the output contains a country, a year, and the position of the county in that year (if within top 4).\n",
    "\n",
    "        BRA, 1962, 1\n",
    "        BRA, 1970, 1\n",
    "        BRA, 1994, 1\n",
    "        BRA, 2002, 1\n",
    "        BRA, 1958, 1\n",
    "        BRA, 1998, 2\n",
    "        BRA, 1950, 2\n",
    "        ...\n",
    "\n",
    "* Start with the given script that cleans up the data a little bit.\n",
    "* No need to re-answer the questions in the Wrangler section, but recompute them to ensure your answers are consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ib.open(\"worldcup.txt\") as f:\n",
    "    world_cup=f.read()\n",
    "    \n",
    "open('/tmp/worldcup.txt','w').write(world_cup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "cat /tmp/worldcup.txt| sed 's/\\[\\[\\([0-9]*\\)[^]]*\\]\\]/\\1/g; s/.*fb|\\([A-Za-z]*\\)}}/\\1/g; s/<sup><\\/sup>//g; s/|bgcolor[^|]*//g; s/|align=center[^|]*//g'>worldcup_clean.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,BRA,1958\n",
      "1,BRA,1962\n",
      "1,BRA,1970\n",
      "1,BRA,1994\n",
      "1,BRA,2002\n",
      "2,BRA,1950\n",
      "2,BRA,1998\n",
      "3,BRA,1938\n",
      "3,BRA,1978\n",
      "4,BRA,1974\n",
      "1,ITA,1934\n",
      "1,ITA,1938\n",
      "1,ITA,1982\n",
      "1,ITA,2006\n",
      "2,ITA,1970\n",
      "2,ITA,1994\n",
      "3,ITA,1990\n",
      "4,ITA,1978\n",
      "1,GER,1954\n",
      "1,GER,1974\n",
      "1,GER,1990\n",
      "2,GER,1966\n",
      "2,GER,1982\n",
      "2,GER,1986\n",
      "2,GER,2002\n",
      "3,GER,1934\n",
      "3,GER,1970\n",
      "3,GER,2006\n",
      "3,GER,2010\n",
      "4,GER,1958\n",
      "1,ARG,1978\n",
      "1,ARG,1986\n",
      "2,ARG,1930\n",
      "2,ARG,1990\n",
      "1,URU,1930\n",
      "1,URU,1950\n",
      "4,URU,1954\n",
      "4,URU,1970\n",
      "4,URU,2010\n",
      "1,FRA,1998\n",
      "2,FRA,2006\n",
      "3,FRA,1958\n",
      "3,FRA,1986\n",
      "4,FRA,1982\n",
      "1,ENG,1966\n",
      "4,ENG,1990\n",
      "1,ESP,2010\n",
      "4,ESP,1950\n",
      "2,NED,1974\n",
      "2,NED,1978\n",
      "2,NED,2010\n",
      "4,NED,1998\n",
      "2,TCH,1934\n",
      "2,TCH,1962\n",
      "2,HUN,1938\n",
      "2,HUN,1954\n",
      "2,SWE,1958\n",
      "3,SWE,1950\n",
      "3,SWE,1994\n",
      "4,SWE,1938\n",
      "3,POL,1974\n",
      "3,POL,1982\n",
      "3,AUT,1954\n",
      "4,AUT,1934\n",
      "3,POR,1966\n",
      "4,POR,2006\n",
      "3,USA,1930\n",
      "3,CHI,1962\n",
      "3,CRO,1998\n",
      "3,TUR,2002\n",
      "4,YUG,1930\n",
      "4,YUG,1962\n",
      "4,URS,1966\n",
      "4,BEL,1986\n",
      "4,BUL,1994\n",
      "4,KOR,2002\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "cat worldcup_clean.txt | tail -171 | head -168 | grep -v '-' | grep -v '|[0-9]\\+\\s\\?$' |\n",
    "awk '/^[A-Z]/{Country=$0} !/^[A-Z]/{print Country, $0}' | sed 's/|[0-9]\\+/|/g' |\n",
    "awk '/.*$/ {info=$0} Begin{i=0} {if(i==4) {i=1} else{i+=1}; print i,\",\"info}' | grep -v '—' | sed 's/[\\(\\)]//g' |\n",
    "awk -F'|' '{count=split($2, years, \",\"); for(i=1;i<=count;i++){print $1, \",\", years[i]} }' |\n",
    "sed 's/\\s\\+//g'\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Questions\n",
    "\n",
    "1. From your experience, briefly discuss the pro and cons between using Data Wrangler as compared to lower levels tools like sed/awk?\n",
    "2. What additional operations would have made using Data Wrangler \"easier\"?\n",
    "\n",
    "#### Note\n",
    "While responding to markdown cells (as the one below), in case you struggle with formatting, just double click any of the markdown cells in the notebook to see how formatting is done. You may also consult the documentation [here](http://jupyter-notebook.readthedocs.io/en/latest/examples/Notebook/Working%20With%20Markdown%20Cells.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***\n",
    "### Add your response below:\n",
    "\n",
    "1. Data Wrangler has a good interaction and GUI with users. It is accessible by most people although they do not understand how to write code to clean dataset. They can accomplish many goals like **_aggregate_**, **_derive_**,**_extract patterns_** from the data only by cliking. Data wrangler also provide cleaning strategies for data users choose or brush. Data Wrangler is also good at Data Visualization by simply clicking buttons. However, sometimes Data Wrangler can not provide what we really want. We can only use the built-in function to manipulate data. For example, I have a clear logic to do the world cup problem. But I spent more than 3 hours to find how to add a new column filling indexes inside group. Finally, I give up the way I want to do. But when I use lower levle tools like sed/awk, I think these basic tools are flexible and I can clearly follow my logic clearly, where Data Wrangler can not provide me with such experience. For coders, I believe lower level tools are more likely understand what they want and help them realize their goal clearly.\n",
    "\n",
    "2. With the experience of problem 1, I think Data Wrangler can embed some Excel functions. For example, they allow users to customize auxiliary or intermediate columns and help them to obtain their goals easily.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 3. Tableau\n",
    "***\n",
    "\n",
    "Finally, you will perform data exploration with Tableau.\n",
    "\n",
    "1. Go to the [Tableau Website](https://www.tableau.com/), and download a demo version of Tableau.\n",
    "\n",
    "    * Tableau gives students a 1 year free license, which can be requested [here](http://www.tableau.com/freeforstudents).\n",
    "\n",
    "2. Connect Tableau to the the OnTime database, hosted on a postgreSQL server we set up for the course.\n",
    "To do so, create a new Tableau workbook.\n",
    "In the workbook, goto data and add a new data source using the credentials below:\n",
    "\n",
    "    * Type: PostgreSQL (you may have to download an additional driver for postgreSQL, [here](https://www.tableau.com/support/drivers)) (only if you can't connect)\n",
    "    * Hostname: pg-001.db.gcloud.instabase.com\n",
    "    * Username: columbia\n",
    "    * Password: B%38Mt5W@M*QU?Ar\n",
    "    * Database: db_fea10998_f88d_4b6e_8f90_a6cd73bac65c\n",
    "    * You should use the table called \"Ontime\".\n",
    "    \n",
    "3. Explore the dataset using Tableau.\n",
    "\n",
    "The aim of this assignment is to understand (1) which flights are the likeliest to be delayed (2) why they are delayed (3) what we could have missed in the data\n",
    "\n",
    "**(1) Which flights are delayed? (You're expected to answer any 2 of the 5 questions below)**\n",
    "- Long flights or short flights?\n",
    "- Which companies?\n",
    "- At what time of the day?\n",
    "- Which state are the most impacted?\n",
    "- Take California. Which cities are the most concerned? And how about NY state?\n",
    "\n",
    "**(2) Why are flights delayed? (You're expected to answer any 2  of the 6 questions below)**\n",
    "- What is the likeliest cause of delays?\n",
    "- Does that depend on the region?\n",
    "- Compare California and NY state\n",
    "- Compare Morning flights and evening flights\n",
    "- Compare weekends and rest of the week\n",
    "- Compare first week of dataset and last week\n",
    "\n",
    "**(3) what we could have missed in the data**\n",
    "- Find three quirky facts about flight delays. Anything goes, as long it involves at least one aggregate and one filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***\n",
    "### Add you response below:\n",
    "\n",
    "***\n",
    "1(a) (Question): Take California. Which cities are the most concerned? And how about NY state?\n",
    "\n",
    "1(a) (Response): For California, I use measure **_Arrdelayminutes_** and take the average according to destination city. Cresent City has the maximum average of delay minutes. Apart from Cresent City, Bakersfied, Redding, which have relatively high averge delay minutes need to be concerned. For NY state, most of destination city has relatively high average of delay minutes, Elmira/Corning has the most delay minutes. Then, Syracuse, New York need to be concerned.\n",
    "\n",
    "***\n",
    "1(b) (Question): Which companies?\n",
    "\n",
    "1(b) (Response): MQ has the largest average arrival delay minutes. Then EV, NK have relatively large average delay minutes.\n",
    "\n",
    "***\n",
    "2(a) (Question): Compare California and NY state\n",
    "\n",
    "2(a) (Response): I make pie chart for NY and CA respectively considering **Carrier Delay**, **Late Aircraft Delay**, **NAS delay**, **Security Delay** and **Weather Delay**. I see for CA the major fact is **Late Aircraft Delay** while for NY the major fact is **NAS Delay**.\n",
    "\n",
    "***\n",
    "2(b) (Question): Compare weekends and rest of the week\n",
    "\n",
    "2(b) (Response): I group **_Dayofweek_** into two groups, i.e. weekend and weekday, and make pie charts as 2(a). The result shows that there are similar distributions between **Weekday** and **Weekend**. For these two groups, the main reason of delay is **Late Aircraft Delay** and the second primary reason is **Carrier Delay**.\n",
    "\n",
    "***\n",
    "3 (Response):\n",
    "- I do a summary of average of departure delay minute according to origin states. I notice Daleware has a significantly high average of delay minute.\n",
    "- I make pie charts for companies respectively. For all companies, **Security Delay** always counts the least, even 0 compared with other kinds of delay.\n",
    "- I draw a bar plot of average weather delay with respect to destination states. According to the bar plot, Iowa has the largest average weather delay while Daleware has the smallest average weather delay.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Submission\n",
    "\n",
    "* Got to 'File' and download this notebook as .ipynb\n",
    "* Rename it as **data\\_processing\\_[your uni].ipynb**\n",
    "* Then submit it on courseworks\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
